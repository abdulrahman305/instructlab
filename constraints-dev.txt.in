# SPDX-License-Identifier: Apache-2.0

torch<2.7.0
vllm<0.9.0

# flash-attn 2.8.0+ is broken for torch 2.6.x.
# See: https://github.com/Dao-AILab/flash-attention/issues/1717
flash-attn<2.8.0

# click 8.2.0+ raises ValueError in vllm_setup_test
# See: https://github.com/instructlab/instructlab/issues/3484
click<8.2.0
